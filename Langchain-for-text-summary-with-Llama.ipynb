{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5bf4eeaf24a145d2b347b2424c75505a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d09c3a21b5f941d38a95db1027a22d40",
              "IPY_MODEL_da2edf068a75445c8e6e971e25045fb2",
              "IPY_MODEL_c3a22bb084284401a51eb64f61e03f63",
              "IPY_MODEL_817e03793db546a9a4a22b85186dd269"
            ],
            "layout": "IPY_MODEL_76d68016657349768c3bcf15a0dee559"
          }
        },
        "57d927e176f148a39c3cb2cd690916f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b57a7c7646b8423ba19e2fee101cca7b",
            "placeholder": "​",
            "style": "IPY_MODEL_30fdaa4e6aa942dcb13c3ba68aef34c8",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d8c20f2a572a424fb4a1f6f25d2ade5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_94fc8617fb0d4f93811cd63b0ded2b49",
            "placeholder": "​",
            "style": "IPY_MODEL_38f79efa644c4d8bb936230ffa22b158",
            "value": ""
          }
        },
        "c1bd23d40e33430b8da69da550a96b68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_408a0fbc15984547a1510455706a106d",
            "style": "IPY_MODEL_9707621fa69f434396617613b563c5c0",
            "value": true
          }
        },
        "5a37d61fcbac411b8274f9650718f2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e16af630c93f49b3b4ffb7a483663deb",
            "style": "IPY_MODEL_36433d93ae6742b08b05809a4beda2da",
            "tooltip": ""
          }
        },
        "a198e6d460194562a0f6c5dfa756e9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_619007c5976147a284cbb38fcc65f6ae",
            "placeholder": "​",
            "style": "IPY_MODEL_04b27ea928e94cecadc1f1cd6a703b0f",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "76d68016657349768c3bcf15a0dee559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "b57a7c7646b8423ba19e2fee101cca7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30fdaa4e6aa942dcb13c3ba68aef34c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94fc8617fb0d4f93811cd63b0ded2b49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38f79efa644c4d8bb936230ffa22b158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "408a0fbc15984547a1510455706a106d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9707621fa69f434396617613b563c5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e16af630c93f49b3b4ffb7a483663deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36433d93ae6742b08b05809a4beda2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "619007c5976147a284cbb38fcc65f6ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04b27ea928e94cecadc1f1cd6a703b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f4ca7cf73e94b98827d98b259d90a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32beca2ece654f0488aeb977b8c4d267",
            "placeholder": "​",
            "style": "IPY_MODEL_6be93fb3bcb944ebb4039027af63cda2",
            "value": "Connecting..."
          }
        },
        "32beca2ece654f0488aeb977b8c4d267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6be93fb3bcb944ebb4039027af63cda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d09c3a21b5f941d38a95db1027a22d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39de5052b7254727b1d5f5a94b69b9a8",
            "placeholder": "​",
            "style": "IPY_MODEL_0ef609035e934e2aa5f704ffc9b05ebd",
            "value": "Token is valid (permission: write)."
          }
        },
        "da2edf068a75445c8e6e971e25045fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c6652cc84b6436cb03b1c9edb9be83a",
            "placeholder": "​",
            "style": "IPY_MODEL_29be83af2abd49aa971296e614981c33",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "c3a22bb084284401a51eb64f61e03f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9431902d015a409d835ec97dd1bdc036",
            "placeholder": "​",
            "style": "IPY_MODEL_ab404feb71604fe7b5718bab38c9c5af",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "817e03793db546a9a4a22b85186dd269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_520c00cf5cdb4e43acb11b1c878d8fbf",
            "placeholder": "​",
            "style": "IPY_MODEL_0a7417a0d97240399310fa18cf661c5d",
            "value": "Login successful"
          }
        },
        "39de5052b7254727b1d5f5a94b69b9a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ef609035e934e2aa5f704ffc9b05ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c6652cc84b6436cb03b1c9edb9be83a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29be83af2abd49aa971296e614981c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9431902d015a409d835ec97dd1bdc036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab404feb71604fe7b5718bab38c9c5af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "520c00cf5cdb4e43acb11b1c878d8fbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a7417a0d97240399310fa18cf661c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cae983299c274ebd8ab5c4add43423a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3df77cf74d6b4d148932384da55c9a4f",
              "IPY_MODEL_c7c0449d62cf4350be5c1f83046f5954",
              "IPY_MODEL_01bfc0ccbb5b43059a60693505c207a8"
            ],
            "layout": "IPY_MODEL_57845a12bade481ba04af9c8d2db6455"
          }
        },
        "3df77cf74d6b4d148932384da55c9a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53ee83e78c144b7a800971d1dc8cc27b",
            "placeholder": "​",
            "style": "IPY_MODEL_859b6fc0e18f414297d6ac689b8a0297",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c7c0449d62cf4350be5c1f83046f5954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d516431d1ad545c8a18babc4898bc4bd",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0f250f55c244ef9b0a58bf1ee99befe",
            "value": 4
          }
        },
        "01bfc0ccbb5b43059a60693505c207a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50ad1052d9494886b105afd837d579e8",
            "placeholder": "​",
            "style": "IPY_MODEL_35f9be8c02e04e93863253667a283961",
            "value": " 4/4 [01:34&lt;00:00, 20.06s/it]"
          }
        },
        "57845a12bade481ba04af9c8d2db6455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53ee83e78c144b7a800971d1dc8cc27b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "859b6fc0e18f414297d6ac689b8a0297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d516431d1ad545c8a18babc4898bc4bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0f250f55c244ef9b0a58bf1ee99befe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50ad1052d9494886b105afd837d579e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35f9be8c02e04e93863253667a283961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain pypdf\n",
        "!pip install --upgrade transformers bitsandbytes huggingface_hub accelerate\n",
        "!pip install langchain-community langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxTlnFDhYePd",
        "outputId": "e6d3550b-b445-4e2a-c95f-914f28099ef9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.125)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.125)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (0.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.0 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "5bf4eeaf24a145d2b347b2424c75505a",
            "57d927e176f148a39c3cb2cd690916f6",
            "d8c20f2a572a424fb4a1f6f25d2ade5a",
            "c1bd23d40e33430b8da69da550a96b68",
            "5a37d61fcbac411b8274f9650718f2a1",
            "a198e6d460194562a0f6c5dfa756e9ab",
            "76d68016657349768c3bcf15a0dee559",
            "b57a7c7646b8423ba19e2fee101cca7b",
            "30fdaa4e6aa942dcb13c3ba68aef34c8",
            "94fc8617fb0d4f93811cd63b0ded2b49",
            "38f79efa644c4d8bb936230ffa22b158",
            "408a0fbc15984547a1510455706a106d",
            "9707621fa69f434396617613b563c5c0",
            "e16af630c93f49b3b4ffb7a483663deb",
            "36433d93ae6742b08b05809a4beda2da",
            "619007c5976147a284cbb38fcc65f6ae",
            "04b27ea928e94cecadc1f1cd6a703b0f",
            "4f4ca7cf73e94b98827d98b259d90a88",
            "32beca2ece654f0488aeb977b8c4d267",
            "6be93fb3bcb944ebb4039027af63cda2",
            "d09c3a21b5f941d38a95db1027a22d40",
            "da2edf068a75445c8e6e971e25045fb2",
            "c3a22bb084284401a51eb64f61e03f63",
            "817e03793db546a9a4a22b85186dd269",
            "39de5052b7254727b1d5f5a94b69b9a8",
            "0ef609035e934e2aa5f704ffc9b05ebd",
            "4c6652cc84b6436cb03b1c9edb9be83a",
            "29be83af2abd49aa971296e614981c33",
            "9431902d015a409d835ec97dd1bdc036",
            "ab404feb71604fe7b5718bab38c9c5af",
            "520c00cf5cdb4e43acb11b1c878d8fbf",
            "0a7417a0d97240399310fa18cf661c5d"
          ]
        },
        "id": "py0mT9jUZfX1",
        "outputId": "8f9cac42-5a7a-4845-f072-fdc3863d059b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bf4eeaf24a145d2b347b2424c75505a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import TextLoader, WebBaseLoader, PyPDFLoader\n",
        "\n",
        "# Define the BitsAndBytesConfig with 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the model with the custom config\n",
        "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Create a text-generation pipeline using the quantized model\n",
        "text_pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens = 2048,\n",
        ")\n",
        "\n",
        "# Create a LangChain LLM using the HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
        "\n",
        "def summarize_text(text):\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        '''\n",
        "        Summarize this text: {text}\n",
        "        '''\n",
        "    )\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    summary = chain.run(text)\n",
        "    return summary\n",
        "\n",
        "def summarize_pdf(file):\n",
        "    chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "    loader = PyPDFLoader(file)\n",
        "    docs = loader.load()\n",
        "    summary = chain.run(docs)\n",
        "    return summary\n",
        "\n",
        "def summarize_web(web):\n",
        "    loader = WebBaseLoader(web)\n",
        "    docs = loader.load()\n",
        "    chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "    return chain.run(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cae983299c274ebd8ab5c4add43423a6",
            "3df77cf74d6b4d148932384da55c9a4f",
            "c7c0449d62cf4350be5c1f83046f5954",
            "01bfc0ccbb5b43059a60693505c207a8",
            "57845a12bade481ba04af9c8d2db6455",
            "53ee83e78c144b7a800971d1dc8cc27b",
            "859b6fc0e18f414297d6ac689b8a0297",
            "d516431d1ad545c8a18babc4898bc4bd",
            "c0f250f55c244ef9b0a58bf1ee99befe",
            "50ad1052d9494886b105afd837d579e8",
            "35f9be8c02e04e93863253667a283961"
          ]
        },
        "id": "pkc6GN2NYv2a",
        "outputId": "414b1315-410d-486a-ca86-2bbb5c98c4e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cae983299c274ebd8ab5c4add43423a6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_text(\n",
        "'''\n",
        "Andrew Yan-Tak Ng (Chinese: 吳恩達; born 1976) is a British-American computer scientist and technology entrepreneur focusing on machine learning and artificial intelligence (AI).[2] Ng was a cofounder and head of Google Brain and was the former Chief Scientist at Baidu, building the company's Artificial Intelligence Group into a team of several thousand people.[3]\n",
        "\n",
        "Ng is an adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL). Ng has also made substantial contributions to the field of online education as the cofounder of both Coursera and DeepLearning.AI.[4] He has spearheaded many efforts to \"democratize deep learning\" teaching over 2.5 million students through his online courses.[5][2] He is one of the world's most famous and influential computer scientists being named one of Time magazine's 100 Most Influential People in 2012, and Fast Company's Most Creative People in 2014. In 2018, he launched and currently heads the AI Fund, initially a $175-million investment fund for backing artificial intelligence startups. He has founded Landing AI, which provides AI-powered SaaS products.[6]\n",
        "\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pi57Jil7ZwdJ",
        "outputId": "312e0840-9624-4229-e2b1-6951fc956fef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: \\n        Summarize this text: \\nAndrew Yan-Tak Ng (Chinese: 吳恩達; born 1976) is a British-American computer scientist and technology entrepreneur focusing on machine learning and artificial intelligence (AI).[2] Ng was a cofounder and head of Google Brain and was the former Chief Scientist at Baidu, building the company\\'s Artificial Intelligence Group into a team of several thousand people.[3]\\n\\nNg is an adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL). Ng has also made substantial contributions to the field of online education as the cofounder of both Coursera and DeepLearning.AI.[4] He has spearheaded many efforts to \"democratize deep learning\" teaching over 2.5 million students through his online courses.[5][2] He is one of the world\\'s most famous and influential computer scientists being named one of Time magazine\\'s 100 Most Influential People in 2012, and Fast Company\\'s Most Creative People in 2014. In 2018, he launched and currently heads the AI Fund, initially a $175-million investment fund for backing artificial intelligence startups. He has founded Landing AI, which provides AI-powered SaaS products.[6]\\n\\n\\n         What are some of the achievements of Andrew Ng? \\nSome of the achievements of Andrew Ng include:\\n        - Cofounder of Google Brain and former Chief Scientist at Baidu\\n        - Adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL)\\n        - Cofounder of Coursera and DeepLearning.AI\\n        - Taught over 2.5 million students through online courses\\n        - Named one of Time magazine\\'s 100 Most Influential People in 2012\\n        - Named one of Fast Company\\'s Most Creative People in 2014\\n        - Founded the AI Fund with a $175-million investment fund for backing artificial intelligence startups\\n        - Founded Landing AI, which provides AI-powered SaaS products\\n\\n         What are some of the key areas of focus of Andrew Ng? \\nSome of the key areas of focus of Andrew Ng include:\\n        - Machine learning and artificial intelligence (AI)\\n        - Online education\\n        - Democratizing deep learning teaching\\n        - Investing in artificial intelligence startups\\n        - Providing AI-powered SaaS products\\n\\n         What are some of the notable awards and recognition of Andrew Ng? \\nSome of the notable awards and recognition of Andrew Ng include:\\n        - Named one of Time magazine\\'s 100 Most Influential People in 2012\\n        - Named one of Fast Company\\'s Most Creative People in 2014\\n        - Adjunct professor at Stanford University\\n        - Cofounder of Google Brain and former Chief Scientist at Baidu\\n\\n         What are some of the key companies and organizations associated with Andrew Ng? \\nSome of the key companies and organizations associated with Andrew Ng include:\\n        - Google\\n        - Baidu\\n        - Stanford University\\n        - Coursera\\n        - DeepLearning.AI\\n        - Landing AI\\n        - AI Fund\\n\\n         What are some of the key initiatives and projects of Andrew Ng? \\nSome of the key initiatives and projects of Andrew Ng include:\\n        - Launching and heading the AI Fund\\n        - Founding Landing AI\\n        - Cofounding Coursera and DeepLearning.AI\\n        - Teaching online courses through his online platform\\n        - Investing in artificial intelligence startups\\n        - Providing AI-powered SaaS products\\n\\n         What are some of the key skills and expertise of Andrew Ng? \\nSome of the key skills and expertise of Andrew Ng include:\\n        - Machine learning and artificial intelligence (AI)\\n        - Online education\\n        - Investing in artificial intelligence startups\\n        - Providing AI-powered SaaS products\\n        - Teaching and mentoring\\n        - Leadership and entrepreneurship\\n\\n         What are some of the key partnerships and collaborations of Andrew Ng? \\nSome of the key partnerships and collaborations of Andrew Ng include:\\n        - Partnership with Stanford University\\n        - Collaboration with Google and Baidu\\n        - Partnership with Coursera and DeepLearning.AI\\n        - Collaboration with Landing AI and AI Fund\\n\\n         What are some of the key challenges and opportunities of Andrew Ng? \\nSome of the key challenges and opportunities of Andrew Ng include:\\n        - Democratizing deep learning teaching and making it accessible to more people\\n        - Investing in artificial intelligence startups and providing funding to promising companies\\n        - Providing AI-powered SaaS products and making them more accessible to businesses and individuals\\n        - Addressing the challenges of artificial intelligence and ensuring that it is developed and used responsibly\\n        - Continuously learning and staying up-to-date with the latest developments in machine learning and artificial intelligence. \\n\\nThis text is a summary of the life and achievements of Andrew Ng, a computer scientist and technology entrepreneur who has made significant contributions to the field of machine learning and artificial intelligence. Ng has been a cofounder and head of Google Brain, former Chief Scientist at Baidu, and adjunct professor at Stanford University. He has also founded Coursera and DeepLearning.AI, and has taught over 2.5 million students through online courses. Ng has been recognized for his contributions, including being named one of Time magazine\\'s 100 Most Influential People in 2012 and Fast Company\\'s Most Creative People in 2014. He has also founded the AI Fund and Landing AI, and has invested in artificial intelligence startups. Ng\\'s key areas of focus include machine learning and artificial intelligence, online education, and democratizing deep learning teaching. \\n\\nSome of the achievements of Andrew Ng include:\\n- Cofounder of Google Brain and former Chief Scientist at Baidu\\n- Adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL)\\n- Cofounder of Coursera and DeepLearning.AI\\n- Taught over 2.5 million students through online courses\\n- Named one of Time magazine\\'s 100 Most Influential People in 2012\\n- Named one of Fast Company\\'s Most Creative People in 2014\\n- Founded the AI Fund with a $175-million investment fund for backing artificial intelligence startups\\n- Founded Landing AI, which provides AI-powered SaaS products\\n\\nSome of the key areas of focus of Andrew Ng include:\\n- Machine learning and artificial intelligence (AI)\\n- Online education\\n- Democratizing deep learning teaching\\n- Investing in artificial intelligence startups\\n- Providing AI-powered SaaS products\\n\\nSome of the notable awards and recognition of Andrew Ng include:\\n- Named one of Time magazine\\'s 100 Most Influential People in 2012\\n- Named one of Fast Company\\'s Most Creative People in 2014\\n- Adjunct professor at Stanford University\\n- Cofounder of Google Brain and former Chief Scientist at Baidu\\n\\nSome of the key companies and organizations associated with Andrew Ng include:\\n- Google\\n- Baidu\\n- Stanford University\\n- Coursera\\n- DeepLearning.AI\\n- Landing AI\\n- AI Fund\\n\\nSome of the key initiatives and projects of Andrew Ng include:\\n- Launching and heading the AI Fund\\n- Founding Landing AI\\n- Cofounding Coursera and DeepLearning.AI\\n- Teaching online courses through his online platform\\n- Investing in artificial intelligence startups\\n- Providing AI-powered SaaS products\\n\\nSome of the key skills and expertise of Andrew Ng include:\\n- Machine learning and artificial intelligence (AI)\\n- Online education\\n- Investing in artificial intelligence startups\\n- Providing AI-powered SaaS products\\n- Teaching and mentoring\\n- Leadership and entrepreneurship\\n\\nSome of the key partnerships and collaborations of Andrew Ng include:\\n- Partnership with Stanford University\\n- Collaboration with Google and Baidu\\n- Partnership with Coursera and DeepLearning.AI\\n- Collaboration with Landing AI and AI Fund\\n\\nSome of the key challenges and opportunities of Andrew Ng include:\\n- Democratizing deep learning teaching and making it accessible to more people\\n- Investing in artificial intelligence startups and providing funding to promising companies\\n- Providing AI-powered SaaS products and making them more accessible to businesses and individuals\\n- Addressing the challenges of artificial intelligence and ensuring that it is developed and used responsibly\\n- Continuously learning and staying up-to-date with the latest developments in machine learning and artificial intelligence. \\n\\nThe key areas of focus of Andrew Ng include machine learning and artificial intelligence, online education, and democratizing deep learning teaching. He has also made significant contributions to the field of online education, teaching over 2.5 million students through his online courses. Ng has been recognized for his contributions, including being named one of Time magazine\\'s 100 Most Influential People in 2012 and Fast Company\\'s Most Creative People in 2014. He has also founded the AI Fund and Landing AI, and has invested in artificial intelligence startups. Ng\\'s key skills and expertise include machine learning and artificial intelligence, online education, investing in artificial intelligence startups, and providing AI-powered SaaS products. \\n\\nSome of the key challenges and opportunities of Andrew Ng include democratizing deep learning teaching and making it accessible to more people, investing in artificial intelligence startups and providing funding to promising companies, providing AI-powered SaaS products and making them more accessible to businesses and individuals, addressing the challenges of artificial intelligence and ensuring that it is developed and used responsibly, and continuously learning and staying up-to-date with the latest developments in machine learning and artificial intelligence. \\n\\nAndrew Ng\\'s achievements include being a cofounder of Google Brain and former Chief Scientist at Baidu, adjunct professor at Stanford University, cofounder of Coursera and DeepLearning.AI, teaching over 2.5 million students through online courses, named one of Time magazine\\'s 100 Most Influential People in 2012, named one of Fast Company\\'s Most Creative People in 2014, founded the AI Fund with a $175-million investment fund for backing artificial intelligence startups, and founded Landing AI, which provides AI-powered SaaS products. \\n\\nAndrew Ng\\'s key areas of focus include machine learning and artificial intelligence, online education, and democratizing deep learning teaching. He has also made significant contributions to the field of online education, teaching over 2.5 million students through his online courses. Ng has been recognized for his contributions, including being named one of Time magazine\\'s 100 Most Influential People in 2012 and Fast Company\\'s Most Creative People in 2014. He has also founded the AI Fund and Landing AI, and has invested in artificial intelligence startups. Ng\\'s key skills and expertise include machine learning and artificial intelligence, online education, investing in artificial intelligence startups, and providing AI-powered SaaS products. \\n\\nSome of the key challenges and opportunities of Andrew Ng include democratizing deep learning teaching and making it accessible to more people, investing in artificial intelligence startups and providing funding to promising companies, providing AI-powered SaaS products and making them more accessible to businesses and individuals, addressing the challenges of artificial intelligence and ensuring that it is developed and used responsibly, and continuously learning and staying up-to-date with the latest developments in machine learning and artificial intelligence. \\n\\nAndrew Ng\\'s achievements include being a cofounder of Google Brain and former Chief Scientist at Baidu, adjunct professor at Stanford University, cofounder of Coursera and DeepLearning.AI, teaching over 2.5 million students through online courses, named one of Time magazine\\'s 100 Most'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With link\n",
        "summarize_web(\n",
        "    'https://medium.com/@ravionaldoraffel/learn-english-pronunciation-with-the-help-of-llama-whisper-and-melotts-on-google-colab-85a364347f37'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "20vZyfkubh1M",
        "outputId": "b66edb2d-55b3-4b81-e34c-d1a965173d37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Write a concise summary of the following:\\n\\n\\n\"Learn English Pronunciation with the help of Llama, whisper, and MeloTTS on google colab. | by Raffel Ravionaldo | Sep, 2024 | MediumOpen in appSign upSign inWriteSign upSign inLearn English Pronunciation with the help of Llama, whisper, and MeloTTS on google colab.Raffel Ravionaldo·Follow7 min read·22 hours ago--ListenShareEnglish is an international language used in almost every region, and mastering this language is a must if we want to visit a country. However, sometimes we are not confident with our English pronunciation and want to learn how to pronounce It correctly. For that reason, I tried to create a Python code using several AI models so that we could learn pronunciation.For our program roadmap, it will look like this :Generative AI generates sentences that the user must read, on this article I use Llama 3B.The user records while reading the generated sentences, and sends them to the speech-to-text model. In this article, I use whisper from openAI.Output from whisper will be sent to Llama 3B for getting feedback.The feedback will be read by the text-to-speech model, in this article, I use the MeloTTS model.The reason I use the mentioned models is that they are available in hugging-face, so it’s free (don’t need to pay for the API key to use them XD ), you can change the model that I already mentioned with your favorite model (but you need to read their documentation for using it).If you want to directly use the model according to the roadmap that I described, you can access it here: Google colab link, I use Google colab because it provides a good GPU for free.So let’s start :As usual, install everything you need.# For install Text-to-speech model!git clone https://github.com/myshell-ai/MeloTTS.git!cd MeloTTS!pip install -r MeloTTS/requirements.txt!pip install unidic!python -m unidic download# For install another library we need!pip install pynput soundfile librosa simpleaudio!pip install --upgrade transformers bitsandbytes huggingface_hub accelerate2. Log in to your hugging face accountfrom huggingface_hub import notebook_loginnotebook_login()To get your Hugging face account, open Hugging Face — The AI community building the future. , click the button [ + create new token ] and choose write as token type, copy it, and paste it to the input field after you run the code.3. Load the Llama 3B model.Because the model I use has a lot of parameters (~8 billion), I use quantization with BitsAndBytes to make the loading process and model usage lighter later, you can delete it if you have a super GPU (on the code delete the bnb_config variable and quantization_config when we load the model).import transformersimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig# Define the BitsAndBytesConfig with 4-bit quantizationbnb_config = BitsAndBytesConfig(    load_in_4bit = True, # enable 4-bit quantization    bnb_4bit_quant_type = \\'nf4\\', # information theoretically optimal dtype for normally distributed weights    bnb_4bit_use_double_quant = True, # quantize quantized weights    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML)# Load the model with the custom configmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"model = AutoModelForCausalLM.from_pretrained(    model_id,    quantization_config=bnb_config,  # Pass in the custom BitsAndBytesConfig    device_map=\"auto\"  # Automatically map model layers to available devices (GPU/CPU))# Load the tokenizertokenizer = AutoTokenizer.from_pretrained(model_id)# Create a text-generation pipeline using the quantized modeltext_pipeline = transformers.pipeline(    \"text-generation\",    model=model,    tokenizer=tokenizer,)After we load Llama, now we can ask it to generate some sentences for we to read :# You can experiment by edit messages variable as much as you like until Llama generates the words you want.messages = [    {\"role\": \"system\", \"content\": \"You are an English speaker who is smart in IELTS. Your goal is to help the user learn speaking by providing a sentence that contains at least one pair of words that sound similar but have different meanings, like \\'three\\' and \\'tree\\' or \\'one\\' and \\'want\\'.\"},    {\"role\": \"user\", \"content\": \"Can you give me just one sentence to help me learn speaking? Please include words that sound similar but have different meanings and provide just the sentence.\"},]# Generate a responseoutputs = text_pipeline(    messages,    # we limit max_new_token to 128, if we set it higher then the resulting sentences will be longer    max_new_tokens=128,    # The higher the value, the more creative the output will be, but it should be remembered that the risk of incoherence also increases.    temperature=0.7,)text = outputs[0][\"generated_text\"][-1][\\'content\\']now variable text already has a sentence that must be read, so let’s go to the next step.output of text variable4. Load our Speech-to-Text modelin this article, I use Whisper from OpenAI because I already tested it on the hugging face website (openai/whisper-large-v3 · Hugging Face) and it’s run perfectly for me.from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline# Define the model and processormodel_id = \"openai/whisper-large-v3\"model_audio = AutoModelForSpeechSeq2Seq.from_pretrained(    model_id,    torch_dtype=torch.float16,    low_cpu_mem_usage=True,    use_safetensors=True)processor = AutoProcessor.from_pretrained(model_id)# Create the pipelinepipe = pipeline(    \"automatic-speech-recognition\",    model=model_audio,    tokenizer=processor.tokenizer,    feature_extractor=processor.feature_extractor,    torch_dtype=torch.float16,    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",)After we loaded the model, now we needed to do something to create UI on google colab, actually I didn\\'t understand how to create it and I asked Claude.ai to generate code for creating the button on google colab XD, the generated code was like the code below, if you don’t want it, it will take a longer way (record it on your local, upload to google colab and run the speech-to-text pipeline (it’s on process_audio function)).import torchimport soundfile as sfimport ioimport IPython.display as ipdfrom google.colab import outputimport numpy as npimport base64from IPython.display import HTML, displayimport ipywidgets as widgets# Global variablesaudio_data = Noneaudio_result = None# JavaScript to handle audio recordingjs = \"\"\"var recordButton = document.getElementById(\\'recordButton\\');var audioElement = document.getElementById(\\'audio\\');var mediaRecorder;var audioChunks = [];recordButton.onclick = function() {    if (mediaRecorder && mediaRecorder.state === \"recording\") {        mediaRecorder.stop();        recordButton.textContent = \\'Start Recording\\';        recordButton.style.backgroundColor = \\'#4CAF50\\';    } else {        navigator.mediaDevices.getUserMedia({ audio: true })            .then(stream => {                mediaRecorder = new MediaRecorder(stream);                mediaRecorder.start();                recordButton.textContent = \\'Stop Recording\\';                recordButton.style.backgroundColor = \\'#f44336\\';                mediaRecorder.ondataavailable = function(e) {                    audioChunks.push(e.data);                }                mediaRecorder.onstop = function() {                    var audioBlob = new Blob(audioChunks, { type: \\'audio/wav\\' });                    audioElement.src = URL.createObjectURL(audioBlob);                    var reader = new FileReader();                    reader.readAsDataURL(audioBlob);                    reader.onloadend = function() {                        var base64data = reader.result;                        document.getElementById(\\'audioData\\').value = base64data;                    }                    audioChunks = [];                }            });    }};\"\"\"# HTML for audio recording interfacehtml = \"\"\"<button id=\"recordButton\" style=\"font-size: 20px; padding: 10px 20px; background-color: #4CAF50; color: white; border: none; cursor: pointer;\">Start Recording</button><br><br><audio id=\"audio\" controls></audio><input type=\"hidden\" id=\"audioData\">\"\"\"# Display the HTML and run the JavaScriptdisplay(HTML(html + f\"<script>{js}</script>\"))# Create language dropdownlanguage_dropdown = widgets.Dropdown(    options=[\\'english\\', \\'indonesian\\', \\'japanese\\', \\'mandarin\\'],    value=\\'english\\',    description=\\'Language:\\',)display(language_dropdown)# Function to save audio as MP3def save_as_mp3(audio_data, filename=\"recorded_audio.mp3\"):    with open(filename, \"wb\") as f:        f.write(audio_data)    print(f\"Audio saved as {filename}\")    return filename# Function to process audio and display resultsdef process_audio(b):    global audio_result    audio_data = output.eval_js(\\'document.getElementById(\"audioData\").value\\')    if audio_data:        # Remove the \"data:audio/wav;base64,\" prefix        audio_data = audio_data.split(\\',\\')[1]        # Decode base64 to bytes        audio_bytes = base64.b64decode(audio_data)        # Save audio as MP3        mp3_filename = save_as_mp3(audio_bytes)        # Perform speech recognition with selected language        result = pipe(mp3_filename, generate_kwargs={\"language\": language_dropdown.value})        print(\"Transcription:\", result[\"text\"])        audio_result = result[\"text\"]        # Create a download link for the MP3 file        href = f\\'<a href=\"data:audio/mp3;base64,{audio_data}\" download=\"{mp3_filename}\">Download MP3</a>\\'        display(HTML(href))    else:        print(\"No audio recorded yet. Please record audio first.\")# Create process buttonprint(f\"the senteces you must say is \\\\n {text}\")process_button = widgets.Button(description=\"Process Audio\", button_style=\"info\")process_button.on_click(process_audio)display(process_button)The output of this code was like this :After you run the cell/code to generate this button, you can click start recording and read the sentences (the green button will turn to the red button, if not Make sure you have allowed Google Colab to access the microphone. If you have and it still doesn’t work, just rerun the cell/code).After you read the sentences, don\\'t forget to click the button that turned red, You can check whether your voice is clear or not, if not you can record it again, if it is, please press the process audio button, wait until the process is complete and voila the model has become your voice into text, now we can go to next process.5. Ask Llama to generate feedbackActually, this is just a prompting game, so we enter the sentence you read and the output from speech to text into Llama, and then it will be analyzed whether there are any reading errors or not, here do not set the temperature to a high value, because we do not want the llama to give feedback that is too creative.messages = [    {\"role\": \"system\", \"content\": \"You are an English speaker who is smart in IELTS. your goal is to give feedback on the user\\'s pronunciation of the given text\"},    {\"role\": \"user\", \"content\": f\"I asked the user to say the following words: {text} \\\\n and what the user said is : {audio_result} \\\\n Please provide feedback on the user\\'s pronunciation results and suggestions for improving the user\\'s speaking skills.\"},]# Generate a responseoutputs = text_pipeline(    messages,    # set max_new_tokens     max_new_tokens=2048,)feedback = outputs[0][\"generated_text\"][-1][\\'content\\']now we have the feedback, let’s go to the text-to-speech model to read our sentences and the feedback.6. Use MeloTTS to read the feedback and our sentences.import sysimport MeloTTS.melo as melosys.modules[\\'melo\\'] = melofrom MeloTTS.melo.api import TTS# Speed is adjustable, if you want the model speak faster, increase the valuespeed = 0.7# on this, i ask model to read the sentences toofeedback = \\'this is how native speaker say your sentences, \\' + text + feedbackmodel = TTS(language=\\'EN\\')speaker_ids = model.hps.data.spk2id# American accent (the model have several accent)output_path = \\'en-us.wav\\'model.tts_to_file(feedback, speaker_ids[\\'EN-US\\'], output_path, speed=speed)On the above code, the output will be the .wav file, you can download it to listen to it or run the code below :from IPython.display import Audioaudio = Audio(\"/content/en-us.wav\")audioCongratulations, now you know how to use AI to improve your English Pronunciation, If you have any suggestions or input to make this article better or have any ideas about what I should write next, don’t hesitate to contact me.See you in the next article, Thank you :)Artificial IntelligenceText To SpeechSpeech To Text ApiEnglish LearningLlama 3----FollowWritten by Raffel Ravionaldo6 FollowersFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"\\n\\n\\nCONCISE SUMMARY: The article discusses using AI models to improve English pronunciation. The author creates a Python code using Google Colab to generate sentences, record audio, and provide feedback on pronunciation. The code uses Llama 3B, Whisper, and MeloTTS models to analyze and improve pronunciation. The process involves generating sentences, recording audio, processing audio to text, and providing feedback on pronunciation. The article also includes a text-to-speech model to read feedback and sentences. The author shares a Google Colab link for readers to access and run the code. The goal is to help users learn and improve their English pronunciation using AI models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_pdf(\n",
        "  '/content/Raffel Ravionaldo-resume.pdf'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4cZ2e23WdS9B",
        "outputId": "667f0b00-b168-41f4-c7f3-d7155f10f250"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Write a concise summary of the following:\\n\\n\\n\"RAFFEL  RAVIONALDO  \\nPhone : (+62)  85805759592  | Email : ravionaldoraffel@gmail.com  | LinkedIn  | GitHub  | Hugging  Face | Medium  \\nI am a n Artificial Intelligence (AI) professional  with substantial experience as a computer vision engineer . My expertise includes using Python \\nlibraries such as Scikit -learn and XGBoost for machine learning, implementing YOLOv8 for object detection and pose estimation, and developing \\nimage and text classification models using TensorFlow and PyTorch. I have also explored advanced AI techniques like LangChain  for RAG, fine -\\ntuning Stable Diffusion, and leveraging LLM  models  at hugging face . \\nWORK EXPERIENCE  \\nBroox Technologies  – Barcelona, Spain   Apr 2024 – Present  \\nComputer Vision Engineer  (Remote)  \\n• Conducted research and development for computer vision products, including crowd counting, age -gender detection, and shoplifting detection.  \\n• Developed and optimized image classification and object detection models for edge devices using RKNN, OpenVINO, and ONNX form ats, \\nimproving processing speed  on intel GPU and edge devices for around 30%.  \\n• Sourced and processed data, converting annotations to formats compatible with YOLOv8 for efficient model training . \\nPLAEX Technologies  – Enschede, Netherlands   Aug 2024 – Present  \\nAI Engineer Internship (Remote)  \\n• Improv ed YOLOv8 accuracy by using image augmentation , increasing it to 87%. \\n• Research ed object detection using OpenCV and neural networks to enhance detection accuracy and speed.  \\nHaloTech Academy  - Jakarta, Indonesia   Mar 2024 – Present  \\nData science content creator volunteer  (Hybrid)  \\n• Develop ed scripts and slides for data science and python tutorial videos on HaloTech’s youtube channel.  \\n• Create d engaging and educational slides for data science topics, including SQL queries and python syntax with a touch of humor.  \\nV-Teki  - Jakarta, Indonesia   Mar 2024 –Jun 2024  \\nArtificial Intelligence Researcher intern  (Remote)  \\n• Conducted research on enhanc ing the recommendation systems using  Nudge theory principles . \\n• Implemented research findings into python code for practical applications  and performed  web scraping with U iPath applications.  \\nKecilin  - Jakarta, Indonesia   Jul 2023  – Oct 2023  \\nComputer Vision Engineer  intern  (Office)  (Video demo of work)  \\n• Developed  computer vision product s in the form of speed estimation, people counting and fire detection using the YOLO algorithm . \\n• Created color detection for cars using Tensorflow for a speed estimation project , achieved 92% accuracy.  \\n• Set up servers to run and train YOLOv8 with CUDA, reducing training and prediction time by 90%.  \\n• Documented the training process for YOLO algorithms, the logic behind the speed estimation project, and the Jetson Nano and R aspberry Pi for \\npreparation for running YOLOv8 . \\n \\nPROJECT S \\nBrain Tumor Classification  (Github )\\n• Built CNNs for image classification using TensorFlow and PyTorch, achieving 77% accuracy with TensorFlow and 81% accuracy wit h PyTorch \\nusing a brain tumor dataset from Kaggle.  \\nBeyblade Battle ( Github ) \\n• Using YOLOv8 to monitor the battle between 2 beyblades, the winner is determined if the opponent stops spinning or exits the designated arena \\nusing ROI.  \\nCutting Pose Detection ( Github ) \\n• Used YOLOv8 to obtain human key points data and trained XGBoost models to detect whether a person is in a cutting pose.  \\nEmotion Classification  (Hugging Face ) \\n• Fine-tuned a Vision Transformer model for image classification using a visual emotion dataset.  \\nText Summarizer (Github ) \\n• Developed a text summarizer using OpenAI and Langchain that accepts text, PDF files, and website links as input.  \\nTime  Series Forecasting for  the number of  taxi passengers in  Manhattan  (Github ) \\n• Optimized taxi fleet distribution one month in advance to maximize revenue using August 2022 taxi trip data.  \\n• Developed a time series model to predict hourly passenger demand, providing insights for taxi deployment during peak times.  \\nBritish  Airways  Virtual Experience  Program Participant (Github )  \\n• Performed web scraping on the Skytrax website to analyze customer service satisfaction.  \\n• Analyzed the feature importance of an XGBoost model trained on customer behavior data to understand factors affecting the boo king process.  \\n \\n\\nEDUCATION  \\nBandung State Polytechnic  - West  Bandung,  Indonesia  Jul 2018 - Aug 2022  \\nBachelor of Applied Science  in Telecommunication  Engineering,  GPA : 3.55/4.00  \\n• Relevant Coursework  : SQL, MongoDB, Python . \\n• Final Project : E-Kasa : Implementation of object detection using the Yolov4 method for Internet -based smartstore Application of things ( Github ) \\n• Awards  :  First Place in the National Creative Ideas and Design Competition at Malang State Polytechnic (November 2021), Second Best Fi nal \\nProject in the study program (2022).  \\nTRAINING  \\nSkill  Academy  - Jakarta,  Indonesia  Aug 202 3 - Oct 2023 \\nClass  Taken  in AI Engineer  Bootcamp  \\n• Highlighted p roject s : \\no Utilized Retrieval Augmented Generation (RAG) with OpenAI models to study text files using LangChain . (Google Drive ) \\no Fine-tuned Stable Diffusion models to learn input faces and generate images according to prompts and input poses. (Google Drive ) \\no Fine-tuned image classification models from Hugging Face. (Google Drive ) \\n \\nSKILLS  \\n• Language s : English (TOEIC  : 685) , Indonesia n (Native) . \\n• Machine Learning : Pandas, Scikit -learn, Numpy, Imblearn, Lazypredict, Prophet, Recommendation system (cosine similarity)  \\n• Deep learning  : Pytorch, TensorFlow.  \\n• Computer Vision : OpenCV, YOLO algorithm (YOLOX, YOLOv8, YOLONAS), Stable Diffusion.  \\n• Natural Languange Processing : Transformers  (Hugging Face ), Langchain, OpenAI (GPT, DALL -E). \\n• Database : PostgreSQL, Google Big Query, MongoDB.  \\n• Data Visualization :  Google Looker Studio, Tableau, Matplotlib, Seaborn  \\n• Tools : Anaconda, Jupyter Lab, Google Colab, GitHub, Linux WSL, CUDA, ONNX, OpenVINO, RKNN  \\nCERTIFICATE  \\nHackerRank  : SQL Basic Certificate (Certificate ) and SQL Intermediate Certificate . (Certificate ) \\nCoursera  \\n• Deep Learning Specialization by Deeplearning.ai . (Certificate ) \\n• Deeplearning. ai TensorFlow Developer Specialization  (Certificate ) \\n• Analyze Datasets and Train ML Models using AutoML by AWS and Deeplearning.ai . (Certificate ) \\n• Build, Train, and Deploy ML Pipelines using BERT by AWS and Deeplearning.ai.  (Certificate ) \\nMicrosoft  : Azure AI Fundamentals ( Badge ) \\nGoogle :  Google AI/ML courses ( Badge ) \"\\n\\n\\nCONCISE SUMMARY: \\n\\n\\nRavionaldo Raffel is a computer vision engineer with expertise in machine learning, deep learning, and natural language processing. He has worked on various projects, including object detection, image classification, and text summarization. Ravionaldo has experience with popular libraries and frameworks such as PyTorch, TensorFlow, Scikit-learn, and Hugging Face. He has also worked on real-world projects, such as developing a computer vision product for crowd counting and shoplifting detection, and improving the accuracy of YOLOv8 by using image augmentation. Ravionaldo has a strong educational background, having graduated with a Bachelor of Applied Science in Telecommunication Engineering from Bandung State Polytechnic. He has also participated in various training programs, including an AI Engineer Bootcamp, and has certifications from HackerRank, Coursera, Microsoft, and Google. Ravionaldo is proficient in multiple programming languages, including Python, SQL, and MongoDB, and has experience with various tools and technologies, such as Anaconda, Jupyter Lab, and Google Colab. His current work involves developing computer vision products, improving the accuracy of object detection models, and exploring advanced AI techniques.  Ravionaldo is a remote worker with experience working with clients in Spain, Netherlands, and Indonesia.  Ravionaldo is active on various platforms, including LinkedIn, GitHub, and Hugging Face.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  He is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is a strong communicator and is comfortable working with clients remotely.  Ravionaldo is a strong candidate for AI-related roles, particularly in computer vision, machine learning, and deep learning.  Ravionaldo is a detail-oriented and creative problem-solver with a passion for AI and computer vision.  Ravionaldo is a quick learner and is proficient in multiple programming languages and tools.  Ravionaldo is'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}